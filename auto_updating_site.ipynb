{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea8e07",
   "metadata": {},
   "outputs": [],
   "source": "# Getting latest HDB resale prices from the API version of this dataset: https://data.gov.sg/collections/189/view\n\nimport requests\nimport time\nimport os\n\ndataset_id = \"d_8b84c4ee58e3cfc0ece0d773c8ca6abc\"  # make sure this is really a CKAN resource_id\nbase_url = \"https://data.gov.sg/api/action/datastore_search\"\n\n# API key from environment variable (set in GitHub Actions secrets)\nAPI_KEY = os.environ.get(\"DATAGOVSG_API_KEY\")\n\n# Retry settings\nMAX_RETRIES = 5\nBASE_DELAY = 10  # Base delay in seconds (API asks for 10 seconds on rate limit)\n\ndef fetch_with_retry(url, params, max_retries=MAX_RETRIES):\n    \"\"\"Fetch data from API with retry logic for transient failures and rate limiting.\"\"\"\n    headers = {}\n    if API_KEY:\n        headers[\"Authorization\"] = API_KEY\n        print(\"Using API key for authentication\")\n    \n    for attempt in range(max_retries):\n        try:\n            print(f\"Requesting (attempt {attempt + 1}/{max_retries})\", url, params)\n            resp = requests.get(url, params=params, headers=headers, timeout=60)\n            \n            # Handle rate limiting (429)\n            if resp.status_code == 429:\n                retry_after = BASE_DELAY * (attempt + 1)  # exponential backoff\n                print(f\"Rate limited (429). Waiting {retry_after} seconds before retry...\")\n                time.sleep(retry_after)\n                continue\n            \n            # Check other HTTP errors\n            if resp.status_code != 200:\n                print(f\"HTTP error {resp.status_code}: {resp.text[:500]}\")\n                if attempt < max_retries - 1:\n                    delay = BASE_DELAY * (attempt + 1)\n                    print(f\"Retrying in {delay} seconds...\")\n                    time.sleep(delay)\n                    continue\n                raise Exception(f\"HTTP error {resp.status_code} after {max_retries} attempts\")\n            \n            data = resp.json()\n            \n            # Check for API-level errors\n            if not data.get(\"success\", False):\n                error_msg = data.get(\"error\", {}).get(\"message\", \"Unknown API error\")\n                print(f\"API error: {error_msg}\")\n                print(f\"Full response: {str(data)[:500]}\")\n                if attempt < max_retries - 1:\n                    delay = BASE_DELAY * (attempt + 1)\n                    print(f\"Retrying in {delay} seconds...\")\n                    time.sleep(delay)\n                    continue\n                raise Exception(f\"API error: {error_msg}\")\n            \n            # Check for expected structure\n            if \"result\" not in data:\n                print(f\"Unexpected API response (no 'result' key): {str(data)[:500]}\")\n                if attempt < max_retries - 1:\n                    delay = BASE_DELAY * (attempt + 1)\n                    print(f\"Retrying in {delay} seconds...\")\n                    time.sleep(delay)\n                    continue\n                raise Exception(f\"API returned unexpected structure after {max_retries} attempts\")\n            \n            return data\n            \n        except requests.exceptions.RequestException as e:\n            print(f\"Network error: {e}\")\n            if attempt < max_retries - 1:\n                delay = BASE_DELAY * (attempt + 1)\n                print(f\"Retrying in {delay} seconds...\")\n                time.sleep(delay)\n                continue\n            raise\n    \n    raise Exception(f\"Failed after {max_retries} attempts\")\n\nall_records = []\noffset = 0\nlimit = 1000  \n\nwhile True:\n    params = {\n        \"resource_id\": dataset_id,\n        \"limit\": limit,\n        \"offset\": offset,\n    }\n    \n    data = fetch_with_retry(base_url, params)\n    \n    # grab this page of rows\n    records = data[\"result\"][\"records\"]\n\n    if not records:\n        break  # no more rows\n\n    all_records.extend(records)\n    \n    # Delay between requests to avoid rate limiting\n    time.sleep(1)\n\n    # move to next page\n    offset += limit\n\nprint(\"Total rows:\", len(all_records))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d3013f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Turn API records into a DataFrame and clean basic columns\n",
    "#    Standard parsing, type-cleaning, and removal of unusable rows.\n",
    "# -------------------------------------------------------------------\n",
    "df = pd.DataFrame(all_records)\n",
    "\n",
    "# Make sure key numeric fields are numeric\n",
    "for col in [\"resale_price\", \"floor_area_sqm\", \"lease_commence_date\"]:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "df[\"month\"] = pd.to_datetime(df[\"month\"], errors=\"coerce\")\n",
    "\n",
    "df = df.dropna(subset=[\"resale_price\", \"floor_area_sqm\", \"lease_commence_date\", \"month\"])\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Helper: z-score within a group\n",
    "#    Computes deviation of a flat from its group's mean.\n",
    "#    Used only as a precursor — final scoring uses extreme-tail logic.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def add_group_zscore(frame, group_cols, value_col, new_col):\n",
    "    grouped = frame.groupby(group_cols, observed=False)[value_col]\n",
    "    mean = grouped.transform(\"mean\")\n",
    "    std = grouped.transform(\"std\").replace(0, np.nan)\n",
    "    frame[new_col] = (frame[value_col] - mean) / std\n",
    "    return frame\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. PRICE SHOCK — how far above similar flats (town + flat_type)\n",
    "#    Measures whether this flat sold unusually high compared to peers\n",
    "#    of the same town and flat type.\n",
    "#    Tiny peer groups (<5 sales) are suppressed to avoid noisy spikes.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "df = add_group_zscore(df,\n",
    "                      [\"town\", \"flat_type\"],\n",
    "                      \"resale_price\",\n",
    "                      \"z_town_flat\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. OUTLIER JUMP — block-level spike (town + block + street_name)\n",
    "#    Detects sudden jumps compared to the block's own price history.\n",
    "#    If historic sample size is tiny (<5), z-scores are nulled.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "df = add_group_zscore(df,\n",
    "                      [\"town\", \"block\", \"street_name\", \"flat_type\"],\n",
    "                      \"resale_price\",\n",
    "                      \"z_block\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5. MARKET DEFIER — high price during a cooling month\n",
    "#    Computes month-on-month median price change.\n",
    "#    If the broader market is falling, high-price sales stand out.\n",
    "#    'cooling_strength' captures how strongly the market was declining.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "monthly_median = df.groupby(\"month\", observed=False)[\"resale_price\"].median().sort_index()\n",
    "monthly_change = monthly_median.pct_change()\n",
    "\n",
    "df = df.sort_values(\"month\")\n",
    "df[\"mo_change\"] = df[\"month\"].map(monthly_change)\n",
    "df[\"cooling_strength\"] = np.where(df[\"mo_change\"] < 0, -df[\"mo_change\"], 0)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 6. UNEXPLAINABLE SPIKE — residual vs expected price model\n",
    "#    Option B: Use a rolling time window for the baseline.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Age of flat at point of resale (approx year-based)\n",
    "df[\"year\"] = df[\"month\"].dt.year\n",
    "df[\"age_years\"] = df[\"year\"] - df[\"lease_commence_date\"]\n",
    "\n",
    "# Coarse bins for size and age\n",
    "df[\"size_bin\"] = pd.cut(\n",
    "    df[\"floor_area_sqm\"],\n",
    "    bins=[0, 40, 60, 80, 100, 130, 200],\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "df[\"age_bin\"] = pd.cut(\n",
    "    df[\"age_years\"],\n",
    "    bins=[0, 10, 20, 30, 40, 50, 60, 80, 120],\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "model_groups = [\"town\", \"flat_type\", \"size_bin\", \"age_bin\"]\n",
    "\n",
    "# ---- NEW: restrict expected-price model to recent years only ----\n",
    "RECENT_YEARS = 5  # adjust if needed\n",
    "\n",
    "latest_year = df[\"year\"].max()\n",
    "cutoff_year = latest_year - RECENT_YEARS + 1\n",
    "\n",
    "# Build baseline table first\n",
    "recent_baseline = df[df[\"year\"] >= cutoff_year].copy()\n",
    "\n",
    "# 6a. Expected price = median of recent comparables\n",
    "expected_lookup = (\n",
    "    recent_baseline\n",
    "    .groupby(model_groups, observed=False)[\"resale_price\"]\n",
    "    .median()\n",
    "    .rename(\"expected_price\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merge expected_price onto df BEFORE making any mask\n",
    "df = df.merge(expected_lookup, on=model_groups, how=\"left\")\n",
    "\n",
    "# Now residuals\n",
    "df[\"price_residual\"] = df[\"resale_price\"] - df[\"expected_price\"]\n",
    "\n",
    "# ---- Recompute mask AFTER merge (this avoids the warning) ----\n",
    "recent_mask = df[\"year\"] >= cutoff_year\n",
    "\n",
    "recent_with_resid = df[recent_mask].copy()\n",
    "\n",
    "# 6b. Residual std from recent years\n",
    "resid_std_lookup = (\n",
    "    recent_with_resid\n",
    "    .groupby(model_groups, observed=False)[\"price_residual\"]\n",
    "    .std()\n",
    "    .replace(0, np.nan)\n",
    "    .rename(\"resid_std\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merge std back into df\n",
    "df = df.merge(resid_std_lookup, on=model_groups, how=\"left\")\n",
    "\n",
    "# Final z_residual\n",
    "df[\"z_residual\"] = df[\"price_residual\"] / df[\"resid_std\"]\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 7. EXTREME-TAIL SCORING (97th percentile)\n",
    "#    Instead of raw z-scores, use only the extreme tail of each group.\n",
    "#    Within each micro-market:\n",
    "#        - Values below 97th percentile = 0\n",
    "#        - Values above it are scaled between (cutoff → max)\n",
    "#    This makes 100-pointers genuinely rare and highlights real outliers.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "TAIL = 0.97\n",
    "\n",
    "def extreme_tail(series, group_keys, tail=TAIL):\n",
    "    \"\"\"\n",
    "    group_keys must be a list like:\n",
    "        [df[\"town\"], df[\"flat_type\"]]\n",
    "        [df[\"town\"], df[\"block\"], df[\"street_name\"]]\n",
    "        [df[\"town\"], df[\"flat_type\"], df[\"size_bin\"], df[\"age_bin\"]]\n",
    "    \"\"\"\n",
    "    s = series.clip(lower=0).fillna(0)\n",
    "\n",
    "    grouped = s.groupby(group_keys, observed=False)\n",
    "\n",
    "    # compute 97th percentile and max within each group\n",
    "    cutoff = grouped.transform(lambda x: x.quantile(tail))\n",
    "    max_val = grouped.transform(\"max\")\n",
    "\n",
    "    # extreme tail only\n",
    "    extreme = (s - cutoff).clip(lower=0)\n",
    "\n",
    "    # denominator\n",
    "    denom = (max_val - cutoff).replace(0, np.nan)\n",
    "\n",
    "    raw = extreme / denom\n",
    "    return raw.fillna(0)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 7a. PRICE SHOCK SCORE\n",
    "#      Extreme-tail version of z_town_flat.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "df[\"price_shock_score\"] = extreme_tail(\n",
    "    df[\"z_town_flat\"],\n",
    "    [df[\"town\"], df[\"flat_type\"]]\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 7b. OUTLIER JUMP SCORE\n",
    "#      Extreme-tail version of block-level deviation.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "df[\"outlier_jump_score\"] = extreme_tail(\n",
    "    df[\"z_block\"],\n",
    "    [df[\"town\"], df[\"block\"], df[\"street_name\"]]\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 7c. MARKET DEFIER SCORE\n",
    "#      Uses cooling_strength × price_shock_raw,\n",
    "#      then applies extreme-tail scoring by town + flat_type.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "df[\"market_defier_raw\"] = df[\"cooling_strength\"] * df[\"z_town_flat\"].clip(lower=0)\n",
    "\n",
    "df[\"market_defier_score\"] = extreme_tail(\n",
    "    df[\"market_defier_raw\"],\n",
    "    [df[\"town\"], df[\"flat_type\"]]\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 7d. UNEXPLAINABLE SPIKE SCORE\n",
    "#      Extreme-tail version of the residual z-score.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "df[\"unexplainable_score\"] = extreme_tail(\n",
    "    df[\"z_residual\"],\n",
    "    [df[\"town\"], df[\"flat_type\"], df[\"size_bin\"], df[\"age_bin\"]]\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 8. WTF SCORE (0–100)\n",
    "#    Editorial blend — weights preserved from original design:\n",
    "#        - Price Shock      → 35%\n",
    "#        - Outlier Jump     → 25%\n",
    "#        - Market Defier    → 15%\n",
    "#        - Unexplainable    → 25%\n",
    "#    Higher scores reflect stronger evidence of a true WTF sale.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "df[\"wtf_score\"] = (\n",
    "    df[\"price_shock_score\"]  * 0.35 +\n",
    "    df[\"outlier_jump_score\"] * 0.25 +\n",
    "    df[\"market_defier_score\"]* 0.15 +\n",
    "    df[\"unexplainable_score\"]* 0.25\n",
    ") * 100\n",
    "\n",
    "df[\"wtf_score\"] = df[\"wtf_score\"].round(2)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 9. WTF BUCKETS — interpretive labels\n",
    "#    0–70   → Not particularly WTF\n",
    "#    70-80  → Mild anomaly\n",
    "#    80–85  → Something is… off\n",
    "#    85–88  → Spicy deviation\n",
    "#    88–100 → Feral pricing\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def wtf_bucket(score):\n",
    "    if score < 70:\n",
    "        return \"Not particularly WTF\"\n",
    "    elif score < 80:\n",
    "        return \"Mild anomaly\"\n",
    "    elif score < 85:\n",
    "        return \"Something is… off\"\n",
    "    elif score < 88:\n",
    "        return \"Spicy deviation\"\n",
    "    else:\n",
    "        return \"Feral pricing\"\n",
    "\n",
    "df[\"wtf_bucket\"] = df[\"wtf_score\"].apply(wtf_bucket)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 10. Recent WTF flats (rolling 1 month)\n",
    "#     Apply threshold and restrict to latest month of sales to keep\n",
    "#     alerts fresh and usable for newsroom monitoring.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "WTF_THRESHOLD = 70\n",
    "\n",
    "wtf_flats = df[df[\"wtf_score\"] >= WTF_THRESHOLD].copy()\n",
    "\n",
    "latest_month = df[\"month\"].max()\n",
    "cutoff_month = latest_month - pd.DateOffset(months=1)\n",
    "\n",
    "recent_wtf_flats = wtf_flats[wtf_flats[\"month\"] >= cutoff_month].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ddc9291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new address column \n",
    "\n",
    "recent_wtf_flats[\"address\"] = (\n",
    "    recent_wtf_flats[\"block\"].astype(str).str.strip() + \" \" +\n",
    "    recent_wtf_flats[\"street_name\"].astype(str).str.strip()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d74472e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geocoding: 652 YISHUN AVE 4, Singapore\n",
      "Geocoding: 813B YISHUN RING RD, Singapore\n",
      "Geocoding: 510 ANG MO KIO AVE 8, Singapore\n",
      "Geocoding: 364B UPP SERANGOON RD, Singapore\n",
      "Geocoding: 275C COMPASSVALE LINK, Singapore\n",
      "Geocoding: 310 CANBERRA RD, Singapore\n",
      "Geocoding: 832 HOUGANG CTRL, Singapore\n",
      "Geocoding: 365D UPP SERANGOON RD, Singapore\n",
      "Geocoding: 365B UPP SERANGOON RD, Singapore\n",
      "Geocoding: 414 SERANGOON CTRL, Singapore\n",
      "Geocoding: 205B COMPASSVALE LANE, Singapore\n",
      "Geocoding: 641 CHOA CHU KANG ST 64, Singapore\n",
      "Geocoding: 217A COMPASSVALE DR, Singapore\n",
      "Geocoding: 445A CLEMENTI AVE 3, Singapore\n",
      "Geocoding: 274C PUNGGOL PL, Singapore\n",
      "Geocoding: 271B PUNGGOL WALK, Singapore\n",
      "Geocoding: 603 ELIAS RD, Singapore\n",
      "Geocoding: 530D PASIR RIS DR 1, Singapore\n",
      "Geocoding: 527C PASIR RIS ST 51, Singapore\n",
      "Geocoding: 325B SUMANG WALK, Singapore\n",
      "Geocoding: 54 MARINE TER, Singapore\n",
      "Geocoding: 271 TOH GUAN RD, Singapore\n",
      "Geocoding: 101A BIDADARI PK DR, Singapore\n",
      "Geocoding: 115C ALKAFF CRES, Singapore\n",
      "Geocoding: 183 JELEBU RD, Singapore\n",
      "Geocoding: 275A BISHAN ST 24, Singapore\n",
      "Geocoding: 273A BISHAN ST 24, Singapore\n",
      "Geocoding: 520B TAMPINES CTRL 8, Singapore\n",
      "Geocoding: 515A TAMPINES CTRL 7, Singapore\n",
      "Geocoding: 405B NORTHSHORE DR, Singapore\n",
      "Geocoding: 994A BUANGKOK LINK, Singapore\n",
      "Geocoding: 221 HOUGANG ST 21, Singapore\n",
      "Geocoding: 856C TAMPINES ST 82, Singapore\n"
     ]
    }
   ],
   "source": [
    "# Geocode the addresses\n",
    "\n",
    "import geocoder\n",
    "\n",
    "# Create empty columns\n",
    "recent_wtf_flats[\"lat\"] = None\n",
    "recent_wtf_flats[\"lng\"] = None\n",
    "\n",
    "for idx, addr in recent_wtf_flats[\"address\"].items():\n",
    "    query = f\"{addr}, Singapore\"\n",
    "    print(\"Geocoding:\", query)\n",
    "\n",
    "    g = geocoder.arcgis(query)\n",
    "\n",
    "    if g.ok and g.latlng:\n",
    "        recent_wtf_flats.at[idx, \"lat\"] = g.latlng[0]\n",
    "        recent_wtf_flats.at[idx, \"lng\"] = g.latlng[1]\n",
    "    else:\n",
    "        print(\"❗Geocoding failed for:\", query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "810071aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_wtf_flats = recent_wtf_flats.rename(columns={\n",
    "    'wtf_bucket': 'Bucket',\n",
    "    'resale_price': 'Price (S$)',\n",
    "    'address': 'Location',\n",
    "    'flat_type': 'Type',\n",
    "    'flat_model': 'Model',\n",
    "    'storey_range': 'Storey',\n",
    "    'floor_area_sqm': 'Area (sqm)',\n",
    "    'remaining_lease': 'Lease left'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5a3341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_wtf_flats.to_csv('wtf_flats.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}